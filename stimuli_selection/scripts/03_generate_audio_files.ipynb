{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Outputs (overwritten after each compilation):\n",
        "- `experiment/resources/audio/con_run1_*.wav`\n",
        "- `experiment/resources/audio/abs_run1_*.wav`\n",
        "- `experiment/resources/audio/con_run2_*.wav`\n",
        "- `experiment/resources/audio/abs_run2_*.wav`\n",
        "- `experiment/resources/audio/base_run1.wav`\n",
        "- `experiment/resources/audio/base_run2.wav`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(WindowsPath('c:/Users/kinga/Documents/Blindbrain/4. Courses/fMRI - design of the experiment and data analysis/cognes-auditory-1back-pilot/experiment/resources/lists'),\n",
              " WindowsPath('c:/Users/kinga/Documents/Blindbrain/4. Courses/fMRI - design of the experiment and data analysis/cognes-auditory-1back-pilot/experiment/resources/audio'),\n",
              " WindowsPath('c:/Users/kinga/Documents/Blindbrain/4. Courses/fMRI - design of the experiment and data analysis/cognes-auditory-1back-pilot/stimuli_selection/models/piper'))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Paths (repo layout)\n",
        "THIS_DIR = Path.cwd()                # stimuli_selection/scripts/\n",
        "ROOT = THIS_DIR.parents[1]           # repo root\n",
        "\n",
        "LISTS_DIR = ROOT / \"experiment\" / \"resources\" / \"lists\"\n",
        "AUDIO_DIR = ROOT / \"experiment\" / \"resources\" / \"audio\"\n",
        "AUDIO_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "STIMSEL = ROOT / \"stimuli_selection\"\n",
        "MODELS_DIR = STIMSEL / \"models\" / \"piper\"\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "LISTS_DIR, AUDIO_DIR, MODELS_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'con_run1.csv': (24, 4),\n",
              " 'abs_run1.csv': (24, 4),\n",
              " 'base_run1.csv': (24, 4),\n",
              " 'con_run2.csv': (24, 4),\n",
              " 'abs_run2.csv': (24, 4),\n",
              " 'base_run2.csv': (24, 4)}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read lists from 01_ notebook outputs\n",
        "files = [\n",
        "    \"con_run1.csv\",\"abs_run1.csv\",\"base_run1.csv\",\n",
        "    \"con_run2.csv\",\"abs_run2.csv\",\"base_run2.csv\",\n",
        "]\n",
        "dfs = {f: pd.read_csv(LISTS_DIR / f) for f in files}\n",
        "{k: v.shape for k,v in dfs.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(WindowsPath('c:/Users/kinga/Documents/Blindbrain/4. Courses/fMRI - design of the experiment and data analysis/cognes-auditory-1back-pilot/stimuli_selection/models/piper/pl_PL-gosia-medium.onnx'),\n",
              " WindowsPath('c:/Users/kinga/Documents/Blindbrain/4. Courses/fMRI - design of the experiment and data analysis/cognes-auditory-1back-pilot/stimuli_selection/models/piper/pl_PL-gosia-medium.onnx.json'))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Piper TTS setup (offline-friendly)\n",
        "# - If you have internet: the cell below can download a Polish model + config from HuggingFace.\n",
        "# - If you are offline: put the two files manually into MODELS_DIR and set MODEL_PATH/CONFIG_PATH.\n",
        "\n",
        "VOICE = \"pl_PL-gosia-medium\"  # good, widely used\n",
        "MODEL_PATH = MODELS_DIR / f\"{VOICE}.onnx\"\n",
        "CONFIG_PATH = MODELS_DIR / f\"{VOICE}.onnx.json\"\n",
        "\n",
        "MODEL_URL = \"https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/pl/pl_PL/gosia/medium/pl_PL-gosia-medium.onnx?download=true\"\n",
        "CONFIG_URL = \"https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/pl/pl_PL/gosia/medium/pl_PL-gosia-medium.onnx.json?download=true\"\n",
        "\n",
        "MODEL_PATH, CONFIG_PATH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('cli', 'piper')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def ensure_piper_installed():\n",
        "    # Prefer CLI if available, else try python package.\n",
        "    for exe in [\"piper\", \"piper.exe\", \"piper-tts\", \"piper-tts.exe\"]:\n",
        "        if shutil.which(exe):\n",
        "            return \"cli\", exe\n",
        "    try:\n",
        "        import piper  # noqa: F401\n",
        "        return \"py\", None\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "backend, cli_exe = ensure_piper_installed()\n",
        "backend, cli_exe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(True, True)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Optional: download the model (requires internet)\n",
        "# If this fails due to network, download manually later.\n",
        "def download_file(url: str, out_path: Path):\n",
        "    import requests\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with requests.get(url, stream=True, timeout=120) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024*1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "\n",
        "if not MODEL_PATH.exists() or not CONFIG_PATH.exists():\n",
        "    print(\"Downloading Piper model/config ...\")\n",
        "    try:\n",
        "        download_file(MODEL_URL, MODEL_PATH)\n",
        "        download_file(CONFIG_URL, CONFIG_PATH)\n",
        "    except Exception as e:\n",
        "        print(\"Download failed:\", repr(e))\n",
        "        print(\"If offline: manually place the model+json into:\", MODELS_DIR)\n",
        "\n",
        "MODEL_PATH.exists(), CONFIG_PATH.exists()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Audio I/O helpers\n",
        "def read_wav(path: Path):\n",
        "    import soundfile as sf\n",
        "    x, sr = sf.read(path, dtype=\"float32\")\n",
        "    if x.ndim > 1:\n",
        "        x = x.mean(axis=1)\n",
        "    return x, sr\n",
        "\n",
        "def write_wav(path: Path, x: np.ndarray, sr: int):\n",
        "    import soundfile as sf\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    sf.write(path, x.astype(np.float32), sr, subtype=\"PCM_16\")\n",
        "\n",
        "def pad_or_trim(x: np.ndarray, target_len: int):\n",
        "    if len(x) == target_len:\n",
        "        return x\n",
        "    if len(x) > target_len:\n",
        "        return x[:target_len]\n",
        "    pad = np.zeros(target_len - len(x), dtype=x.dtype)\n",
        "    return np.concatenate([x, pad])\n",
        "\n",
        "def prepend_silence(x: np.ndarray, sr: int, silence_s: float = 0.25) -> np.ndarray:\n",
        "    n_silence = int(round(silence_s * sr))\n",
        "    silence = np.zeros(n_silence, dtype=x.dtype)\n",
        "    return np.concatenate([silence, x])\n",
        "\n",
        "def slow_down(x: np.ndarray, rate: float = 0.5) -> np.ndarray:\n",
        "    # rate < 1.0 = slower\n",
        "    return librosa.effects.time_stretch(x, rate=rate)\n",
        "\n",
        "# Try to ensure soundfile is present\n",
        "try:\n",
        "    import soundfile as sf  # noqa\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Missing dependency: soundfile. Install it in your notebook env: pip install soundfile\") from e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TTS synthesis (Piper)\n",
        "def synth_piper_cli(text: str, out_wav: Path, model: Path, config: Path, exe: str):\n",
        "    # Piper CLI reads text from stdin\n",
        "    cmd = [exe, \"--model\", str(model), \"--config\", str(config), \"--output_file\", str(out_wav)]\n",
        "    p = subprocess.run(cmd, input=text.encode(\"utf-8\"), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    if p.returncode != 0:\n",
        "        raise RuntimeError(f\"Piper CLI failed: {p.stderr.decode('utf-8', errors='ignore')[:500]}\")\n",
        "\n",
        "def synth_piper_py(text: str, out_wav: Path, model: Path, config: Path):\n",
        "    # Python API may vary by version; try common entrypoints.\n",
        "    try:\n",
        "        from piper.voice import PiperVoice\n",
        "        voice = PiperVoice.load(str(model), str(config))\n",
        "        voice.synthesize(text, str(out_wav))\n",
        "        return\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        from piper import PiperVoice  # type: ignore\n",
        "        voice = PiperVoice.load(str(model), str(config))\n",
        "        voice.synthesize(text, str(out_wav))\n",
        "        return\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Piper Python API not available. Install CLI or use the CLI backend.\") from e\n",
        "\n",
        "def synth(text: str, out_wav: Path):\n",
        "    if not MODEL_PATH.exists() or not CONFIG_PATH.exists():\n",
        "        raise FileNotFoundError(\"Missing Piper model/config. Put them into MODELS_DIR or run the download cell.\")\n",
        "    if backend == \"cli\":\n",
        "        synth_piper_cli(text, out_wav, MODEL_PATH, CONFIG_PATH, cli_exe)\n",
        "    elif backend == \"py\":\n",
        "        synth_piper_py(text, out_wav, MODEL_PATH, CONFIG_PATH)\n",
        "    else:\n",
        "        raise RuntimeError(\"Piper not installed. Easiest: pip install piper (or install CLI).\")\n",
        "\n",
        "# quick smoke-test (comment out if you want)\n",
        "# test_path = AUDIO_DIR / \"_tts_test.wav\"\n",
        "# synth(\"To jest test.\", test_path)\n",
        "# test_path.exists()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(96,\n",
              " [('rakieta',\n",
              "   WindowsPath('c:/Users/kinga/Documents/Blindbrain/4. Courses/fMRI - design of the experiment and data analysis/cognes-auditory-1back-pilot/experiment/resources/audio/con_run1_001.wav')),\n",
              "  ('prysznic',\n",
              "   WindowsPath('c:/Users/kinga/Documents/Blindbrain/4. Courses/fMRI - design of the experiment and data analysis/cognes-auditory-1back-pilot/experiment/resources/audio/con_run1_002.wav')),\n",
              "  ('owca',\n",
              "   WindowsPath('c:/Users/kinga/Documents/Blindbrain/4. Courses/fMRI - design of the experiment and data analysis/cognes-auditory-1back-pilot/experiment/resources/audio/con_run1_003.wav'))])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Collect all target (word, file) pairs from the list CSVs\n",
        "def collect_pairs(df: pd.DataFrame) -> list[tuple[str, Path]]:\n",
        "    pairs = []\n",
        "    for _, row in df.iterrows():\n",
        "        stim = str(row.get(\"stimFile\", \"\")).strip()\n",
        "        word = str(row.get(\"word\", \"\")).strip()\n",
        "        if not stim or stim.lower() == \"nan\":\n",
        "            continue\n",
        "        # stimFile is relative to experiment/, e.g. \"resources/audio/xxx.wav\"\n",
        "        out = ROOT / \"experiment\" / stim.replace(\"\\\\\", \"/\")\n",
        "        # Skip BASE here (handled separately)\n",
        "        if str(row.get(\"condition\",\"\")).upper() == \"BASE\":\n",
        "            continue\n",
        "        pairs.append((word, out))\n",
        "    # unique by output path (in case of repeats)\n",
        "    seen = set()\n",
        "    uniq = []\n",
        "    for w,p in pairs:\n",
        "        if p in seen:\n",
        "            continue\n",
        "        seen.add(p)\n",
        "        uniq.append((w,p))\n",
        "    return uniq\n",
        "\n",
        "pairs = []\n",
        "pairs += collect_pairs(dfs[\"con_run1.csv\"])\n",
        "pairs += collect_pairs(dfs[\"abs_run1.csv\"])\n",
        "pairs += collect_pairs(dfs[\"con_run2.csv\"])\n",
        "pairs += collect_pairs(dfs[\"abs_run2.csv\"])\n",
        "\n",
        "len(pairs), pairs[:3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(np.float64(0.42993197278911566),\n",
              " np.float64(0.5779591836734694),\n",
              " np.float64(0.8072562358276644),\n",
              " 96)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Pass 1: synthesize to temp, measure durations, choose a common target duration.\n",
        "TMP_DIR = AUDIO_DIR / \"_tmp_raw\"\n",
        "TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "raw_paths = []\n",
        "durations = []\n",
        "\n",
        "for i, (word, out_path) in enumerate(pairs, start=1):\n",
        "    tmp = TMP_DIR / out_path.name\n",
        "    synth(word, tmp)  # overwrite temp\n",
        "    x, sr = read_wav(tmp)\n",
        "\n",
        "    # silence\n",
        "    x = slow_down(x, rate=2)\n",
        "    x = prepend_silence(x, sr, silence_s=0.25)\n",
        "    raw_paths.append((tmp, out_path))\n",
        "    durations.append(len(x) / sr)\n",
        "\n",
        "durations = np.array(durations, dtype=float)\n",
        "durations.min(), np.median(durations), durations.max(), len(durations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.5779591836734694, 0.75, 16538)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Choose target duration suggested by the data (snap to 0.25s grid)\n",
        "GRID = np.array([0.75, 1.00, 1.25, 1.50], dtype=float)  # extend if needed\n",
        "median_d = float(np.median(durations))\n",
        "target_d = float(GRID[np.argmin(np.abs(GRID - median_d))])\n",
        "\n",
        "SR_TARGET = 22050  # gosia model card uses 22050 Hz\n",
        "TARGET_SAMPLES = int(round(target_d * SR_TARGET))\n",
        "\n",
        "median_d, target_d, TARGET_SAMPLES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'AUDIO_WRITTEN'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Pass 2: pad/trim to common duration, write to final locations (overwrite)\n",
        "for tmp, out_path in raw_paths:\n",
        "    x, sr = read_wav(tmp)\n",
        "\n",
        "    # slow down + silence\n",
        "    x = slow_down(x, rate=0.5)\n",
        "    x = prepend_silence(x, sr, silence_s=0.25)\n",
        "    if sr != SR_TARGET:\n",
        "        # resample if needed\n",
        "        x = librosa.resample(x, orig_sr=sr, target_sr=SR_TARGET)\n",
        "        sr = SR_TARGET\n",
        "    x2 = pad_or_trim(x, TARGET_SAMPLES)\n",
        "    write_wav(out_path, x2, sr)\n",
        "\n",
        "# cleanup temp\n",
        "for tmp, _ in raw_paths:\n",
        "    try:\n",
        "        tmp.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "try:\n",
        "    TMP_DIR.rmdir()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\"AUDIO_WRITTEN\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(WindowsPath('c:/Users/kinga/Documents/Blindbrain/4. Courses/fMRI - design of the experiment and data analysis/cognes-auditory-1back-pilot/experiment/resources/audio/base_run1.wav'),\n",
              " WindowsPath('c:/Users/kinga/Documents/Blindbrain/4. Courses/fMRI - design of the experiment and data analysis/cognes-auditory-1back-pilot/experiment/resources/audio/base_run2.wav'),\n",
              " 'przyjaźń tragedia jaszczurka lustro idiota zagadka')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# BASELINE: noise-vocoded speech (standard degraded-speech baseline)\n",
        "# We generate a short phrase from selected words, synthesize it, vocode it, and crop/pad to target duration.\n",
        "\n",
        "def noise_vocode(x: np.ndarray, sr: int, n_bands: int = 6, seed: int = 0) -> np.ndarray:\n",
        "    from scipy.signal import butter, filtfilt, hilbert\n",
        "\n",
        "    rng_local = np.random.default_rng(seed)\n",
        "\n",
        "    x = x.astype(np.float32)\n",
        "    x = x / (np.max(np.abs(x)) + 1e-8)\n",
        "\n",
        "    # log-spaced bands\n",
        "    f_lo, f_hi = 80.0, min(8000.0, sr / 2 - 100.0)\n",
        "    edges = np.logspace(np.log10(f_lo), np.log10(f_hi), n_bands + 1)\n",
        "\n",
        "    y = np.zeros_like(x)\n",
        "    for b in range(n_bands):\n",
        "        low, high = edges[b], edges[b + 1]\n",
        "\n",
        "        sos_b, sos_a = butter(\n",
        "            4,\n",
        "            [low / (sr / 2), high / (sr / 2)],\n",
        "            btype=\"bandpass\",\n",
        "        )\n",
        "\n",
        "        band = filtfilt(sos_b, sos_a, x)\n",
        "        env = np.abs(hilbert(band))\n",
        "\n",
        "        noise = rng_local.standard_normal(len(x)).astype(np.float32)\n",
        "        noise_band = filtfilt(sos_b, sos_a, noise)\n",
        "\n",
        "        y += env * noise_band\n",
        "\n",
        "    y = y / (np.max(np.abs(y)) + 1e-8)\n",
        "    return y\n",
        "\n",
        "\n",
        "def make_base(run: int, out_name: str, n_words: int = 6, n_bands: int = 6, seed: int = 1234):\n",
        "    rng_local = np.random.default_rng(seed + run)\n",
        "\n",
        "    # take words from that run's CON+ABS lists\n",
        "    words = []\n",
        "    for f in [f\"con_run{run}.csv\", f\"abs_run{run}.csv\"]:\n",
        "        words += dfs[f][\"word\"].astype(str).tolist()\n",
        "\n",
        "    rng_local.shuffle(words)\n",
        "    phrase = \" \".join(words[:n_words])\n",
        "\n",
        "    tmp = AUDIO_DIR / f\"_base_tmp_run{run}.wav\"\n",
        "    synth(phrase, tmp)\n",
        "\n",
        "    x, sr = read_wav(tmp)\n",
        "    if sr != SR_TARGET:\n",
        "        import librosa\n",
        "        x = librosa.resample(x, orig_sr=sr, target_sr=SR_TARGET)\n",
        "        sr = SR_TARGET\n",
        "\n",
        "    # slow down + silence BEFORE vocoding\n",
        "    x = slow_down(x, rate=0.5)\n",
        "    x = prepend_silence(x, sr, silence_s=0.25)\n",
        "\n",
        "    y = noise_vocode(x, sr=sr, n_bands=n_bands, seed=seed + run)\n",
        "\n",
        "    y = pad_or_trim(y, TARGET_SAMPLES)\n",
        "\n",
        "    out_path = AUDIO_DIR / out_name\n",
        "    write_wav(out_path, y, sr)\n",
        "\n",
        "    try:\n",
        "        tmp.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return out_path, phrase\n",
        "\n",
        "\n",
        "base1_path, base1_phrase = make_base(1, \"base_run1.wav\", n_words=6, n_bands=6)\n",
        "base2_path, base2_phrase = make_base(2, \"base_run2.wav\", n_words=6, n_bands=6)\n",
        "\n",
        "base1_path, base2_path, base1_phrase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(24, 24, True)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Quick audit\n",
        "def list_audio(prefix: str):\n",
        "    return sorted([p.name for p in AUDIO_DIR.glob(prefix)])\n",
        "\n",
        "len(list_audio(\"con_run1_*.wav\")), len(list_audio(\"abs_run1_*.wav\")), (AUDIO_DIR/\"base_run1.wav\").exists()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
